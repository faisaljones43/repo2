{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "\n",
    "from typing import Annotated\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tmdbv3api import TMDb, Genre\n",
    "from openai import AsyncOpenAI\n",
    "from tmdbv3api import TMDb\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.contents import FunctionCallContent, FunctionResultContent, StreamingTextContent\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from tmdbv3api import TMDb, Discover\n",
    "from tmdbv3api import Movie\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tmdbv3api import TMDb, Discover\n",
    "import os, json\n",
    "from tmdbv3api import TMDb, Genre, Discover\n",
    "import semantic_kernel as sk\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb = TMDb()\n",
    "tmdb.api_key=os.environ.get('TMDB_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TMDb with your API key\n",
    "tmdb = TMDb()\n",
    "tmdb.api_key ='2fa30f6a1d22eb80c6dc9cac9cc67bdc'\n",
    "# Fetch the official list of movie genres\n",
    "genre_client = Genre()\n",
    "all_genres = genre_client.movie_list()\n",
    "\n",
    "# Build a nameâ†’ID lookup (lowercased keys)\n",
    "GENRE_MAP = {g.name.lower(): g.id for g in all_genres}\n",
    "\n",
    "# Create a Discover instance for querying movies\n",
    "discover = Discover()\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.environ.get(\"GITHUB_TOKEN\"), \n",
    "    base_url=\"https://models.inference.ai.azure.com/\",\n",
    ")\n",
    "\n",
    "# Create an AI Service that will be used by the `ChatCompletionAgent`\n",
    "chat_completion_service = OpenAIChatCompletion(\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    async_client=client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(\n",
    "    genre_id: int,\n",
    "    preference: str = \"newer\",\n",
    "    top_n: int    = 5,\n",
    "    weight_pop: float = 0.5,\n",
    "    weight_vote: float = 0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch page 1 for `genre_id`, score by weighted(popularity, vote_average),\n",
    "    filter by age (â‰¤5 yrs new vs >5 yrs old), return top_n dicts.\n",
    "    \"\"\"\n",
    "    current_year = datetime.now().year\n",
    "    raw = discover.discover_movies({\n",
    "        \"with_genres\": genre_id,\n",
    "        \"sort_by\":     \"popularity.desc\",\n",
    "        \"page\":        1\n",
    "    })\n",
    "    scored = []\n",
    "    for m in raw:\n",
    "        try:\n",
    "            year = int(m.release_date[:4])\n",
    "        except:\n",
    "            continue\n",
    "        age = current_year - year\n",
    "        if preference == \"newer\" and age > 5:  continue\n",
    "        if preference == \"older\" and age <= 5: continue\n",
    "        score = weight_pop * m.popularity + weight_vote * m.vote_average\n",
    "        scored.append({\"title\": m.title, \"year\": year, \"score\": score})\n",
    "    return sorted(scored, key=lambda x: x[\"score\"], reverse=True)[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoviePlugin:\n",
    "    @kernel_function(description=\"Recommend movies by genre (name or ID), date preference, and number of results\")\n",
    "    def recommend(\n",
    "        self,\n",
    "        genre: str,\n",
    "        preference: str = \"newer\",\n",
    "        top_n: str    = \"5\"\n",
    "    ) -> str:\n",
    "        if genre.isdigit():\n",
    "            gid = int(genre)\n",
    "        else:\n",
    "            gid = GENRE_MAP.get(genre.lower())\n",
    "            if gid is None:\n",
    "                return (\n",
    "                    f\"Unknown genre '{genre}'. \"\n",
    "                    f\"Available: {', '.join(GENRE_MAP.keys())}\"\n",
    "                )\n",
    "        try:\n",
    "            n = int(top_n)\n",
    "        except:\n",
    "            n = 5\n",
    "\n",
    "        recs = recommend_movies(genre_id=gid, preference=preference, top_n=n)\n",
    "\n",
    "        return \"\\n\".join(\n",
    "            f\"{i+1}. {m['title']} ({m['year']}) â€” score={m['score']:.2f}\"\n",
    "            for i, m in enumerate(recs)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ChatCompletionAgent(\n",
    "    service=chat_completion_service, \n",
    "    plugins=[MoviePlugin()],\n",
    "    name=\"MovieAgent\",\n",
    "    instructions=\"You will identify the preferred genres that the customer wants based on their prompt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceResponseException",
     "evalue": "(\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20736 seconds before retrying.', 'details': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20736 seconds before retrying.'}}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:87\u001b[0m, in \u001b[0;36mOpenAIHandler._send_completion_request\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m     86\u001b[0m         settings_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 87\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings_dict)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2028\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   2027\u001b[0m validate_response_format(response_format)\n\u001b[1;32m-> 2028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   2029\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2030\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   2031\u001b[0m         {\n\u001b[0;32m   2032\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   2033\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   2034\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m   2035\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   2036\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   2038\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   2039\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   2040\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   2041\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   2042\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   2043\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m   2044\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   2045\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   2046\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m   2047\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   2048\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m   2049\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   2050\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   2051\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   2052\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   2053\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   2054\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   2055\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   2056\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   2057\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   2058\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   2059\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   2060\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   2061\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   2062\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m   2063\u001b[0m         },\n\u001b[0;32m   2064\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[0;32m   2065\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m   2066\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[0;32m   2067\u001b[0m     ),\n\u001b[0;32m   2068\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   2069\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   2070\u001b[0m     ),\n\u001b[0;32m   2071\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   2072\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2073\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m   2074\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\openai\\_base_client.py:1742\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1739\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1740\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1741\u001b[0m )\n\u001b[1;32m-> 1742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\openai\\_base_client.py:1549\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20736 seconds before retrying.', 'details': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20736 seconds before retrying.'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mServiceResponseException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 77\u001b[0m\n\u001b[0;32m     69\u001b[0m         html_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<div style=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmargin-bottom:20px\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<div style=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfont-weight:bold\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_name\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAssistant\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:</div>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<div style=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmargin-left:20px; white-space:pre-wrap\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(full_response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</div></div><hr>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     75\u001b[0m         display(HTML(html_output))\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m current_function_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     22\u001b[0m argument_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m agent\u001b[38;5;241m.\u001b[39minvoke_stream(\n\u001b[0;32m     25\u001b[0m     messages\u001b[38;5;241m=\u001b[39muser_input,\n\u001b[0;32m     26\u001b[0m     thread\u001b[38;5;241m=\u001b[39mthread,\n\u001b[0;32m     27\u001b[0m ):\n\u001b[0;32m     28\u001b[0m     thread \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mthread\n\u001b[0;32m     29\u001b[0m     agent_name \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\agent_diagnostics\\decorators.py:39\u001b[0m, in \u001b[0;36mtrace_agent_invocation.<locals>.wrapper_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mdescription:\n\u001b[0;32m     37\u001b[0m     span\u001b[38;5;241m.\u001b[39mset_attribute(gen_ai_attributes\u001b[38;5;241m.\u001b[39mAGENT_DESCRIPTION, agent\u001b[38;5;241m.\u001b[39mdescription)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m invoke_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\agents\\chat_completion\\chat_completion_agent.py:419\u001b[0m, in \u001b[0;36mChatCompletionAgent.invoke_stream\u001b[1;34m(self, messages, thread, on_intermediate_message, arguments, kernel, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m role \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    418\u001b[0m response_builder: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response_list \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m response_list:\n\u001b[0;32m    421\u001b[0m         role \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrole\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py:261\u001b[0m, in \u001b[0;36mChatCompletionClientBase.get_streaming_chat_message_contents\u001b[1;34m(self, chat_history, settings, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m all_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamingChatMessageContent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    260\u001b[0m function_call_returned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m messages \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_get_streaming_chat_message_contents(\n\u001b[0;32m    262\u001b[0m     chat_history, settings, request_index\n\u001b[0;32m    263\u001b[0m ):\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\model_diagnostics\\decorators.py:165\u001b[0m, in \u001b[0;36mtrace_streaming_chat_completion.<locals>.inner_trace_streaming_chat_completion.<locals>.wrapper_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(completion_func)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_decorator\u001b[39m(\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncGenerator[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamingChatMessageContent\u001b[39m\u001b[38;5;124m\"\u001b[39m], Any]:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m streaming_chat_message_contents \u001b[38;5;129;01min\u001b[39;00m completion_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    166\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m streaming_chat_message_contents\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_chat_completion_base.py:110\u001b[0m, in \u001b[0;36mOpenAIChatCompletionBase._inner_get_streaming_chat_message_contents\u001b[1;34m(self, chat_history, settings, function_invoke_attempt)\u001b[0m\n\u001b[0;32m    107\u001b[0m settings\u001b[38;5;241m.\u001b[39mmessages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat_history_for_request(chat_history)\n\u001b[0;32m    108\u001b[0m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_id\n\u001b[1;32m--> 110\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(settings)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, AsyncStream):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceInvalidResponseError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an AsyncStream[ChatCompletionChunk] response.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:59\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mTEXT \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mCHAT:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_completion_request(settings)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mEMBEDDING:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIEmbeddingPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:104\u001b[0m, in \u001b[0;36mOpenAIHandler._send_completion_request\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    101\u001b[0m         ex,\n\u001b[0;32m    102\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m         ex,\n\u001b[0;32m    107\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[1;31mServiceResponseException\u001b[0m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20736 seconds before retrying.', 'details': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20736 seconds before retrying.'}}\"))"
     ]
    }
   ],
   "source": [
    "user_inputs = [\n",
    "    \"I like comedy movies recommend me some.\",\n",
    "    \"I don't like that one, please show me horror.\",\n",
    "]\n",
    "\n",
    "async def main():\n",
    "    thread: ChatHistoryAgentThread | None = None\n",
    "\n",
    "    for user_input in user_inputs:\n",
    "        html_output = (\n",
    "            f\"<div style='margin-bottom:10px'>\"\n",
    "            f\"<div style='font-weight:bold'>User:</div>\"\n",
    "            f\"<div style='margin-left:20px'>{user_input}</div></div>\"\n",
    "        )\n",
    "\n",
    "        agent_name = None\n",
    "        full_response: list[str] = []\n",
    "        function_calls: list[str] = []\n",
    "\n",
    "        # Buffer to reconstruct streaming function call\n",
    "        current_function_name = None\n",
    "        argument_buffer = \"\"\n",
    "\n",
    "        async for response in agent.invoke_stream(\n",
    "            messages=user_input,\n",
    "            thread=thread,\n",
    "        ):\n",
    "            thread = response.thread\n",
    "            agent_name = response.name\n",
    "            content_items = list(response.items)\n",
    "\n",
    "            for item in content_items:\n",
    "                if isinstance(item, FunctionCallContent):\n",
    "                    if item.function_name:\n",
    "                        current_function_name = item.function_name\n",
    "\n",
    "                    # Accumulate arguments (streamed in chunks)\n",
    "                    if isinstance(item.arguments, str):\n",
    "                        argument_buffer += item.arguments\n",
    "                elif isinstance(item, FunctionResultContent):\n",
    "                    # Finalize any pending function call before showing result\n",
    "                    if current_function_name:\n",
    "                        formatted_args = argument_buffer.strip()\n",
    "                        try:\n",
    "                            parsed_args = json.loads(formatted_args)\n",
    "                            formatted_args = json.dumps(parsed_args)\n",
    "                        except Exception:\n",
    "                            pass  \n",
    "\n",
    "                        function_calls.append(f\"Calling function: {current_function_name}({formatted_args})\")\n",
    "                        current_function_name = None\n",
    "                        argument_buffer = \"\"\n",
    "\n",
    "                    function_calls.append(f\"\\nFunction Result:\\n\\n{item.result}\")\n",
    "                elif isinstance(item, StreamingTextContent) and item.text:\n",
    "                    full_response.append(item.text)\n",
    "\n",
    "        if function_calls:\n",
    "            html_output += (\n",
    "                \"<div style='margin-bottom:10px'>\"\n",
    "                \"<details>\"\n",
    "                \"<summary style='cursor:pointer; font-weight:bold; color:#0066cc;'>Function Calls (click to expand)</summary>\"\n",
    "                \"<div style='margin:10px; padding:10px; background-color:#f8f8f8; \"\n",
    "                \"border:1px solid #ddd; border-radius:4px; white-space:pre-wrap; font-size:14px; color:#333;'>\"\n",
    "                f\"{chr(10).join(function_calls)}\"\n",
    "                \"</div></details></div>\"\n",
    "            )\n",
    "\n",
    "        html_output += (\n",
    "            \"<div style='margin-bottom:20px'>\"\n",
    "            f\"<div style='font-weight:bold'>{agent_name or 'Assistant'}:</div>\"\n",
    "            f\"<div style='margin-left:20px; white-space:pre-wrap'>{''.join(full_response)}</div></div><hr>\"\n",
    "        )\n",
    "\n",
    "        display(HTML(html_output))\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Quiz Genres ===\n",
      " â€¢ Action   â†’ ID = 28\n",
      " â€¢ Adventure â†’ ID = 12\n",
      " â€¢ Animation â†’ ID = 16\n",
      " â€¢ Comedy   â†’ ID = 35\n",
      " â€¢ Crime    â†’ ID = 80\n",
      " â€¢ Documentary â†’ ID = 99\n",
      " â€¢ Drama    â†’ ID = 18\n",
      " â€¢ Family   â†’ ID = 10751\n",
      " â€¢ Fantasy  â†’ ID = 14\n",
      " â€¢ History  â†’ ID = 36\n",
      " â€¢ Horror   â†’ ID = 27\n",
      " â€¢ Music    â†’ ID = 10402\n",
      " â€¢ Mystery  â†’ ID = 9648\n",
      " â€¢ Romance  â†’ ID = 10749\n",
      " â€¢ Science Fiction â†’ ID = 878\n",
      " â€¢ Tv Movie â†’ ID = 10770\n",
      " â€¢ Thriller â†’ ID = 53\n",
      " â€¢ War      â†’ ID = 10752\n",
      " â€¢ Western  â†’ ID = 37\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "GENRES_TO_USE = list(GENRE_MAP.keys())\n",
    "# 1) Validate that each chosen genre exists in GENRE_MAP\n",
    "missing = [g for g in GENRES_TO_USE if g not in GENRE_MAP]\n",
    "if missing:\n",
    "    raise ValueError(f\"These genres are not in GENRE_MAP: {missing}\")\n",
    "\n",
    "# 2) Build a dict of name â†’ ID for quick lookup later\n",
    "SELECTED_GENRES = {g: GENRE_MAP[g] for g in GENRES_TO_USE}\n",
    "\n",
    "# 3) Print out your selections for confirmation\n",
    "print(\"=== Quiz Genres ===\")\n",
    "for name, gid in SELECTED_GENRES.items():\n",
    "    print(f\" â€¢ {name.title():<8} â†’ ID = {gid}\")\n",
    "print(\"===================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from tmdbv3api import Discover, Movie\n",
    "\n",
    "# Assumes youâ€™ve already done:\n",
    "#   tmdb = TMDb()\n",
    "#   tmdb.api_key = YOUR_TMDB_API_KEY\n",
    "discover = Discover()\n",
    "movie_api = Movie()\n",
    "\n",
    "class MovieTinderPlugin:\n",
    "    @kernel_function(\n",
    "        description=\"Step 1: Given genres JSON, return top 2 movie IDs, titles & years per genre\"\n",
    "    )\n",
    "    def fetch_options(self, genres_json: str) -> str:\n",
    "        # 1) Parse the incoming genres list\n",
    "        try:\n",
    "            genres = json.loads(genres_json) if isinstance(genres_json, str) else genres_json\n",
    "        except json.JSONDecodeError:\n",
    "            genres = [genres_json]\n",
    "        # 2) For each genre name, look up its ID and fetch top 2 by popularity\n",
    "        result: dict[str, list[dict]] = {}\n",
    "        for g in genres:\n",
    "            gid = GENRE_MAP.get(g.lower())\n",
    "            if not gid:\n",
    "                continue\n",
    "            raw = list(discover.discover_movies({\n",
    "                \"with_genres\": gid,\n",
    "                \"sort_by\":     \"popularity.desc\",\n",
    "                \"page\":        1\n",
    "            }))[:2]\n",
    "            result[g.lower()] = [\n",
    "                {\n",
    "                    \"id\":           m.id,\n",
    "                    \"title\":        m.title,\n",
    "                    \"release_year\": (m.release_date or \"\").split(\"-\")[0]\n",
    "                }\n",
    "                for m in raw\n",
    "            ]\n",
    "        return json.dumps(result)\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Step 3: Given options JSON, pick one movie ID per genre\"\n",
    "    )\n",
    "    def quiz_preferences(self, options_json: str) -> str:\n",
    "        opts = json.loads(options_json)\n",
    "        # autoâ€pick the first movie for each genre for this prototype\n",
    "        picks: list[int] = []\n",
    "        for movies in opts.values():\n",
    "            if isinstance(movies, list) and movies:\n",
    "                picks.append(movies[0][\"id\"])\n",
    "        return json.dumps(picks)\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Step 4: Given picked IDs JSON, build a 25â€movie candidate pool per pick\"\n",
    "    )\n",
    "    def build_candidates(self, picks_json: str) -> str:\n",
    "        # 1) parse or coerce into Python list\n",
    "        try:\n",
    "            parsed = json.loads(picks_json)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed = [int(picks_json)]\n",
    "        # 2) flatten if it came in as dict\n",
    "        if isinstance(parsed, dict):\n",
    "            raw_ids = []\n",
    "            for v in parsed.values():\n",
    "                  raw_ids += v if isinstance(v, list) else [v]\n",
    "        elif isinstance(parsed, list):\n",
    "            raw_ids = parsed\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid picks_input: {picks_json!r}\")\n",
    "\n",
    "        # 3) fetch top 25 popular movies for each pick\n",
    "        pool: dict[str, list[dict]] = {}\n",
    "        for pid in raw_ids:\n",
    "            movies = list(discover.discover_movies({\n",
    "                \"sort_by\": \"popularity.desc\",\n",
    "                \"page\":    1\n",
    "            }))[:25]\n",
    "            pool[str(pid)] = [\n",
    "                {\n",
    "                    \"id\":           m.id,\n",
    "                    \"title\":        m.title,\n",
    "                    \"release_year\": (m.release_date or \"\").split(\"-\")[0]\n",
    "                }\n",
    "                for m in movies\n",
    "            ]\n",
    "        return json.dumps(pool)\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Step 5: From picks and candidates JSON, return 5 full movie recommendations\"\n",
    "    )\n",
    "    def recommend(self, picks_json: str, candidates_json: str) -> str:\n",
    "        picks = json.loads(picks_json)\n",
    "        candidates = json.loads(candidates_json)\n",
    "        # flatten & unique\n",
    "        all_ids = []\n",
    "        for cid_list in (candidates.values() if isinstance(candidates, dict) else []):\n",
    "            for mid in cid_list:\n",
    "                if mid not in all_ids:\n",
    "                    all_ids.append(mid)\n",
    "        # sample up to 5\n",
    "        chosen = random.sample(all_ids, k=min(5, len(all_ids)))\n",
    "\n",
    "        # fetch details for each\n",
    "        recs: list[dict] = []\n",
    "        for mid in chosen:\n",
    "            m = movie_api.details(mid)\n",
    "            recs.append({\n",
    "                \"id\":           m.id,\n",
    "                \"title\":        m.title,\n",
    "                \"release_year\": (m.release_date or \"\").split(\"-\")[0],\n",
    "                \"overview\":     m.overview or \"\",\n",
    "                \"popularity\":   m.popularity,\n",
    "                \"vote_average\": m.vote_average,\n",
    "            })\n",
    "        return json.dumps(recs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "    api_key=os.environ.get(\"GITHUB_TOKEN\"), \n",
    "    base_url=\"https://models.inference.ai.azure.com/\",\n",
    ")\n",
    "\n",
    "# Create an AI Service that will be used by the `ChatCompletionAgent`\n",
    "chat_completion_service = OpenAIChatCompletion(\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    async_client=client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create the Kernel\n",
    "from semantic_kernel import Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# 2) Register your MovieTinderPlugin under the namespace \"MovieTinder\"\n",
    "#    (this makes its @kernel_function methods available as MovieTinder.fetch_options, etc.)\n",
    "kernel.add_plugin(\n",
    "    MovieTinderPlugin(),\n",
    "    plugin_name=\"MovieTinder\"\n",
    ")\n",
    "\n",
    "# 3) Build your ChatCompletionAgent as before\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "    service = chat_completion_service,\n",
    "    plugins = [MovieTinderPlugin()],\n",
    "    name    = \"MovieAgent\",\n",
    "    instructions = \"\"\"\n",
    "You are MovieAgent. To answer any movie request, follow exactly:\n",
    "\n",
    "1) CALL_FUNCTION: MovieTinder.fetch_options(genres='<JSON list of genres>')\n",
    "2) Then CALL_FUNCTION: MovieTinder.quiz_preferences(options='<that JSON>')\n",
    "3) Then CALL_FUNCTION: MovieTinder.build_candidates(picks='<that JSON>')\n",
    "4) Finally CALL_FUNCTION: MovieTinder.recommend(picks='<step3 JSON>', candidates='<step4 JSON>')\n",
    "\n",
    "Return only the final JSON array of 5 full movie recommendation objects\n",
    "(each with id, title, release_year, overview, popularity, vote_average), and nothing else.\n",
    "\"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬ Welcome to MovieTinder!\n",
      "\n",
      "Agent: Which genres are you in the mood for today? (e.g. Action, Comedy)\n"
     ]
    },
    {
     "ename": "ServiceResponseException",
     "evalue": "(\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20664 seconds before retrying.', 'details': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20664 seconds before retrying.'}}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:87\u001b[0m, in \u001b[0;36mOpenAIHandler._send_completion_request\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m     86\u001b[0m         settings_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 87\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings_dict)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2028\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   2027\u001b[0m validate_response_format(response_format)\n\u001b[1;32m-> 2028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   2029\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2030\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   2031\u001b[0m         {\n\u001b[0;32m   2032\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   2033\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   2034\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m   2035\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   2036\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   2038\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   2039\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   2040\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   2041\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   2042\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   2043\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m   2044\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   2045\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   2046\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m   2047\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   2048\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m   2049\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   2050\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   2051\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   2052\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   2053\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   2054\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   2055\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   2056\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   2057\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   2058\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   2059\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   2060\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   2061\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   2062\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m   2063\u001b[0m         },\n\u001b[0;32m   2064\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[0;32m   2065\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m   2066\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[0;32m   2067\u001b[0m     ),\n\u001b[0;32m   2068\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   2069\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   2070\u001b[0m     ),\n\u001b[0;32m   2071\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   2072\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2073\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m   2074\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\openai\\_base_client.py:1742\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1739\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1740\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1741\u001b[0m )\n\u001b[1;32m-> 1742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\openai\\_base_client.py:1549\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20664 seconds before retrying.', 'details': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20664 seconds before retrying.'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mServiceResponseException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m user_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()  \u001b[38;5;66;03m# e.g. Action, Comedy\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Stream the agentâ€™s first response (should CALL fetch_options)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m agent\u001b[38;5;241m.\u001b[39minvoke_stream(messages\u001b[38;5;241m=\u001b[39muser_msg, thread\u001b[38;5;241m=\u001b[39mthread):\n\u001b[0;32m     13\u001b[0m     thread \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mthread\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mitems:\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\agent_diagnostics\\decorators.py:39\u001b[0m, in \u001b[0;36mtrace_agent_invocation.<locals>.wrapper_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mdescription:\n\u001b[0;32m     37\u001b[0m     span\u001b[38;5;241m.\u001b[39mset_attribute(gen_ai_attributes\u001b[38;5;241m.\u001b[39mAGENT_DESCRIPTION, agent\u001b[38;5;241m.\u001b[39mdescription)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m invoke_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\agents\\chat_completion\\chat_completion_agent.py:419\u001b[0m, in \u001b[0;36mChatCompletionAgent.invoke_stream\u001b[1;34m(self, messages, thread, on_intermediate_message, arguments, kernel, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m role \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    418\u001b[0m response_builder: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response_list \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m response_list:\n\u001b[0;32m    421\u001b[0m         role \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrole\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py:261\u001b[0m, in \u001b[0;36mChatCompletionClientBase.get_streaming_chat_message_contents\u001b[1;34m(self, chat_history, settings, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m all_messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamingChatMessageContent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    260\u001b[0m function_call_returned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m messages \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_get_streaming_chat_message_contents(\n\u001b[0;32m    262\u001b[0m     chat_history, settings, request_index\n\u001b[0;32m    263\u001b[0m ):\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\model_diagnostics\\decorators.py:165\u001b[0m, in \u001b[0;36mtrace_streaming_chat_completion.<locals>.inner_trace_streaming_chat_completion.<locals>.wrapper_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(completion_func)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_decorator\u001b[39m(\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncGenerator[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreamingChatMessageContent\u001b[39m\u001b[38;5;124m\"\u001b[39m], Any]:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m streaming_chat_message_contents \u001b[38;5;129;01min\u001b[39;00m completion_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    166\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m streaming_chat_message_contents\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_chat_completion_base.py:110\u001b[0m, in \u001b[0;36mOpenAIChatCompletionBase._inner_get_streaming_chat_message_contents\u001b[1;34m(self, chat_history, settings, function_invoke_attempt)\u001b[0m\n\u001b[0;32m    107\u001b[0m settings\u001b[38;5;241m.\u001b[39mmessages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat_history_for_request(chat_history)\n\u001b[0;32m    108\u001b[0m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_id\n\u001b[1;32m--> 110\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(settings)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, AsyncStream):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceInvalidResponseError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an AsyncStream[ChatCompletionChunk] response.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:59\u001b[0m, in \u001b[0;36mOpenAIHandler._send_request\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mTEXT \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mCHAT:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_completion_request(settings)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_model_type \u001b[38;5;241m==\u001b[39m OpenAIModelTypes\u001b[38;5;241m.\u001b[39mEMBEDDING:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIEmbeddingPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\conda-meta\\.conda\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:104\u001b[0m, in \u001b[0;36mOpenAIHandler._send_completion_request\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    101\u001b[0m         ex,\n\u001b[0;32m    102\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m service failed to complete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m         ex,\n\u001b[0;32m    107\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[1;31mServiceResponseException\u001b[0m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", RateLimitError(\"Error code: 429 - {'error': {'code': 'RateLimitReached', 'message': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20664 seconds before retrying.', 'details': 'Rate limit of 150 per 86400s exceeded for UserByModelByDay. Please wait 20664 seconds before retrying.'}}\"))"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.agents import ChatHistoryAgentThread\n",
    "\n",
    "thread: ChatHistoryAgentThread | None = None\n",
    "\n",
    "print(\"ðŸŽ¬ Welcome to MovieTinder!\\n\")\n",
    "\n",
    "# â”€â”€â”€ STEP 1: Ask for genres â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"Agent: Which genres are you in the mood for today? (e.g. Action, Comedy)\")\n",
    "user_msg = input()  # e.g. Action, Comedy\n",
    "\n",
    "# Stream the agentâ€™s first response (should CALL fetch_options)\n",
    "async for resp in agent.invoke_stream(messages=user_msg, thread=thread):\n",
    "    thread = resp.thread\n",
    "    for item in resp.items:\n",
    "        if hasattr(item, \"text\") and item.text:\n",
    "            print(item.text, end=\"\", flush=True)\n",
    "        elif hasattr(item, \"function_name\"):\n",
    "            print(f\"\\nâ†’ Calling {item.function_name}({item.arguments})\\n\")\n",
    "        elif hasattr(item, \"result\"):\n",
    "            print(f\"\\nâ† Function result:\\n{item.result}\\n\")\n",
    "print()  # blank line\n",
    "\n",
    "# â”€â”€â”€ STEPS 2â€“5: quiz & recommend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "while True:\n",
    "    # The agent will have printed something like â€œ1) Foo vs. Bar â†’ â€\n",
    "    choice = input()  # type â€œ1â€ or â€œ2â€ and press Enter\n",
    "    async for resp in agent.invoke_stream(messages=choice, thread=thread):\n",
    "        thread = resp.thread\n",
    "        for item in resp.items:\n",
    "            if hasattr(item, \"text\") and item.text:\n",
    "                print(item.text, end=\"\", flush=True)\n",
    "            elif hasattr(item, \"function_name\"):\n",
    "                print(f\"\\nâ†’ Calling {item.function_name}({item.arguments})\\n\")\n",
    "            elif hasattr(item, \"result\"):\n",
    "                print(f\"\\nâ† Function result:\\n{item.result}\\n\")\n",
    "    print()  # blank line\n",
    "\n",
    "    # Break once the agent has returned a JSON array of recommendations\n",
    "    final_text = \"\".join(\n",
    "        it.text for it in resp.items\n",
    "        if hasattr(it, \"text\") and it.text\n",
    "    ).strip()\n",
    "    if final_text.startswith(\"[\") and final_text.endswith(\"]\"):\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
